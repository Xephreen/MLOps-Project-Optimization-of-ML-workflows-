{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3882155-cd67-4fa0-914e-89716fab8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import requests\n",
    "import kfp.dsl as dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4d0eb09-d765-44e6-9c94-47fb9c6f2d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages\\pyyaml-5.4.1-py3.12-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kfp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Version: 1.8.18\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: https://github.com/kubeflow/pipelines\n",
      "Author: The Kubeflow Authors\n"
     ]
    }
   ],
   "source": [
    "!pip show kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b727b128-9f97-4d48-89f9-23eda2ed2f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author-email: \n",
      "License: \n",
      "Location: C:\\Users\\user\\AppData\\Roaming\\Python\\Python312\\site-packages\n"
     ]
    }
   ],
   "source": [
    "def prepare_data():\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.utils import resample\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    print(\"---- Inside prepare_data component ----\")\n",
    "    \n",
    "    # Chargement des données\n",
    "    num_rows_to_read = 100000\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/Xephreen/Churn_data/main/Master_table.csv?token=GHSAT0AAAAAACRE22CC6BYWQNEKXREY53XKZUCZ6DQ\", nrows=num_rows_to_read)\n",
    "    \n",
    "    # Trier le DataFrame par la colonne \"Idclient\"\n",
    "    df = data.sort_values(by=\"Idclient\")\n",
    "    \n",
    "    # Afficher un aperçu des premières lignes du DataFrame\n",
    "    print(\"Aperçu des premières lignes du DataFrame :\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Afficher les informations sur les colonnes et les types de données\n",
    "    print(\"Informations sur les colonnes et les types de données :\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Afficher un résumé statistique des variables numériques\n",
    "    print(\"Résumé statistique des variables numériques :\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Afficher les valeurs uniques dans chaque colonne\n",
    "    print(\"Valeurs uniques dans chaque colonne :\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].unique()}\")\n",
    "    \n",
    "    # Afficher le nombre de valeurs manquantes dans chaque colonne\n",
    "    print(\"Nombre de valeurs manquantes dans chaque colonne :\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Afficher la distribution de la variable cible\n",
    "    print(\"Distribution de la variable cible :\")\n",
    "    print(df['Churn_next_trim'].value_counts())\n",
    "\n",
    "    # Exclure les colonnes non numériques de la sélection des variables\n",
    "    numeric_cols = [col for col in df.select_dtypes(include=['int64', 'float64']).columns if col != 'Idclient' and col != 'Churn_next_trim' and col !='ACTIVE_RATIO']\n",
    "\n",
    "    # Gestion des valeurs aberrantes\n",
    "    for col in numeric_cols:\n",
    "        # Calculer le premier et le troisième quartile (Q1 et Q3)\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        # Calculer l'écart interquartile (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # Définir les limites inférieure et supérieure pour détecter les valeurs aberrantes\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Remplacer les valeurs aberrantes par la médiane\n",
    "        df[col] = df[col].apply(lambda x: df[col].median() if x < lower_bound or x > upper_bound else x)\n",
    "    \n",
    "    # Séparation des classes pour rééchantillonnage\n",
    "    df_class_0 = df[df['Churn_next_trim'] == 0]\n",
    "    df_class_1 = df[df['Churn_next_trim'] == 1]\n",
    "    n_class_0 = len(df_class_0)\n",
    "    n_class_1 = len(df_class_1)\n",
    "    n_class_1_resampled = int(0.3 * n_class_0 / 0.7)\n",
    "    df_class_1_resampled = resample(df_class_1, replace=True, n_samples=n_class_1_resampled, random_state=42)\n",
    "    df_resampled = pd.concat([df_class_0, df_class_1_resampled])\n",
    "    \n",
    "    \n",
    "    # Affichage des distributions avant et après rééchantillonnage\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df['Churn_next_trim'].value_counts().plot(kind='bar', color=['blue', 'orange'])\n",
    "    plt.title('Distribution des classes avant rééchantillonnage')\n",
    "    plt.xlabel('Classe')\n",
    "    plt.ylabel(\"Nombre d'observations\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    df_resampled['Churn_next_trim'].value_counts().plot(kind='bar', color=['blue', 'orange'])\n",
    "    plt.title('Distribution des classes après rééchantillonnage')\n",
    "    plt.xlabel('Classe')\n",
    "    plt.ylabel(\"Nombre d'observations\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualiser la distribution des variables numériques avec des histogrammes\n",
    "    df[numeric_cols].hist(figsize=(15, 10))\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualiser la distribution des variables catégorielles avec des diagrammes en barres\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    palette = sns.color_palette(\"tab10\", 7)\n",
    "    for i, col in enumerate(cat_cols):\n",
    "        counts = df[col].value_counts()\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(counts.index, counts.values, color=palette)\n",
    "        plt.title(f\"Diagramme en barres pour {col}\", fontsize=14)\n",
    "        plt.xlabel(col, fontsize=12)\n",
    "        plt.ylabel(\"Fréquence\", fontsize=12)\n",
    "        plt.legend(bars, counts.index, fontsize=10)\n",
    "        plt.show()\n",
    "    \n",
    "    # Tracer les distributions univariées de chaque variable catégorielle\n",
    "    for column in cat_cols:\n",
    "        sns.displot(df[column], kde=True)\n",
    "        plt.title(f\"Distribution de {column}\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Calculer et visualiser la matrice de corrélation\n",
    "    correlation_matrix = df.select_dtypes(include=['int64', 'float64']).corr()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(\"Matrice de corrélation entre les variables numériques\", fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    # Sélection des caractéristiques importantes\n",
    "    X = df[numeric_cols]\n",
    "    y = df['Churn_next_trim']\n",
    "    lasso = Lasso(alpha=0.01)\n",
    "    selector_lasso = SelectFromModel(lasso)\n",
    "    selector_lasso.fit(X, y)\n",
    "    selected_features_lasso = X.columns[selector_lasso.get_support()]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    importances = rf.feature_importances_\n",
    "    selector_rf = SelectFromModel(rf, threshold=0.05)\n",
    "    selector_rf.fit(X, y)\n",
    "    selected_features_rf = X.columns[selector_rf.get_support()]\n",
    "    \n",
    "    print(\"Caractéristiques sélectionnées par Lasso :\")\n",
    "    print(selected_features_lasso)\n",
    "    print(\"\\nCaractéristiques sélectionnées par RandomForest :\")\n",
    "    print(selected_features_rf)\n",
    "\n",
    "    # Création de la dataframe avec les colonnes sélectionnées\n",
    "    df_resampled = df_resampled[['Idclient'] + list(selected_features_rf) + ['Churn_next_trim']]\n",
    "\n",
    "    # Enregistrement du DataFrame final\n",
    "    df_resampled = df_resampled.sort_values(by='Idclient')\n",
    "    df_resampled.to_csv(f'data/final_df.csv', index=False)\n",
    "    print(\"\\n ---- data csv is saved to PV location /data/final_df.csv ----\")\n",
    "        \n",
    "    # Retourner le DataFrame final pour une utilisation ultérieure si nécessaire\n",
    "    return df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd784022-0c9b-46c6-9fb3-8ba67c92dbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires: absl-py, click, cloudpickle, Deprecated, docstring-parser, fire, google-api-core, google-api-python-client, google-auth, google-cloud-storage, jsonschema, kfp-pipeline-spec, kfp-server-api, kubernetes, protobuf, pydantic, PyYAML, requests-toolbelt, strip-hints, tabulate, typer, uritemplate\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "def train_test_split():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    print(\"---- Inside train_test_split component ----\")\n",
    "    final_data = pd.read_csv(f'data/final_df.csv')\n",
    "    target_column = 'Churn_next_trim'\n",
    "    X = final_data.loc[:, (final_data.columns != target_column) & (final_data.columns != 'Idclient')]\n",
    "    y = final_data.loc[:, final_data.columns == target_column]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify = y, random_state=47)\n",
    "    \n",
    "    np.save(f'data/X_train.npy', X_train)\n",
    "    np.save(f'data/X_test.npy', X_test)\n",
    "    np.save(f'data/y_train.npy', y_train)\n",
    "    np.save(f'data/y_test.npy', y_test)\n",
    "    \n",
    "    print(\"\\n---- X_train ----\")\n",
    "    print(\"\\n\")\n",
    "    print(X_train)\n",
    "    \n",
    "    print(\"\\n---- X_test ----\")\n",
    "    print(\"\\n\")\n",
    "    print(X_test)\n",
    "    \n",
    "    print(\"\\n---- y_train ----\")\n",
    "    print(\"\\n\")\n",
    "    print(y_train)\n",
    "    \n",
    "    print(\"\\n---- y_test ----\")\n",
    "    print(\"\\n\")\n",
    "    print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76474b1f-4290-46b4-8f59-a2fe232c1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_basic_classifier_with_early_stopping(model_choice):\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pickle\n",
    "    \n",
    "    print(\"---- Inside training_basic_classifier_with_early_stopping component ----\")\n",
    "    \n",
    "    # Charger les données\n",
    "    X_train = np.load('data/X_train.npy', allow_pickle=True)\n",
    "    y_train = np.load('data/y_train.npy', allow_pickle=True)\n",
    "    \n",
    "    # Diviser les données en ensemble d'entraînement et ensemble de validation\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Sélectionner le modèle\n",
    "    if model_choice == 'Logistic Regression':\n",
    "        classifier = LogisticRegression(max_iter=500, solver='liblinear', penalty='l2', C=1.0)\n",
    "    elif model_choice == 'Decision Trees':\n",
    "        classifier = DecisionTreeClassifier(max_depth=5)\n",
    "    elif model_choice == 'Random Forest':\n",
    "        classifier = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "    elif model_choice == 'Support Vector Machines':\n",
    "        classifier = SVC(probability=True, C=1.0, kernel='linear')\n",
    "    elif model_choice == 'k-Nearest Neighbors':\n",
    "        classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "    elif model_choice == 'Naive Bayes':\n",
    "        classifier = GaussianNB()\n",
    "    elif model_choice == 'XGBoost':\n",
    "        classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100, max_depth=5)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model choice. Please select one of the following: 'Logistic Regression', 'Decision Trees', 'Random Forests', 'Support Vector Machines', 'k-Nearest Neighbors', 'Naive Bayes', 'XGBoost'\")\n",
    "    \n",
    "    # Entraîner le modèle avec détection précoce pour XGBoost\n",
    "    if model_choice == 'XGBoost':\n",
    "        eval_set = [(X_train_split, y_train_split), (X_val_split, y_val_split)]\n",
    "        classifier.fit(X_train_split, y_train_split, eval_set=eval_set, early_stopping_rounds=10, verbose=True)\n",
    "    else:\n",
    "        classifier.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # Sauvegarder le modèle entraîné\n",
    "    with open(f'data/{model_choice}.pkl', 'wb') as f:\n",
    "        pickle.dump(classifier, f)\n",
    "    \n",
    "    print(f\"\\n{model_choice} classifier is trained and saved to PV location /data/{model_choice}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3514dc2a-7cde-49c1-81c9-d112b6fcdb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_data(model_path, X_test_path, y_pred_path):\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "    print(\"---- Inside predict_on_test_data component ----\")\n",
    "    \n",
    "    # Charger le modèle\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Charger les données de test\n",
    "    X_test = np.load(X_test_path, allow_pickle=True)\n",
    "    \n",
    "    # Faire des prédictions sur les données de test\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Sauvegarder les prédictions\n",
    "    np.save(y_pred_path, y_pred)\n",
    "    \n",
    "    print(\"\\n---- Predicted classes ----\")\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afc97b99-a6a9-450b-97a8-fe7704967572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_test_path, y_pred_path):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    print(\"---- Inside get_metrics component ----\")\n",
    "    \n",
    "    # Charger les vraies étiquettes et les prédictions\n",
    "    y_test = np.load(y_test_path, allow_pickle=True)\n",
    "    y_pred = np.load(y_pred_path, allow_pickle=True)\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='micro')\n",
    "    recall = recall_score(y_test, y_pred, average='micro')\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    auc = roc_auc_score(y_test, y_pred, average='micro', multi_class='ovr')\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nModel Metrics:\")\n",
    "    print(f\"Accuracy: {round(acc, 2)}\")\n",
    "    print(f\"Precision: {round(prec, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "    print(f\"F1-Score: {round(f1, 2)}\")\n",
    "    print(f\"AUC: {round(auc, 2)}\")\n",
    "    print(f\"MCC: {round(mcc, 2)}\")\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7033c8c-d913-4cff-a380-1470d733aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(X_train_path, y_train_path, model_choice):\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from scipy.stats import randint, uniform\n",
    "\n",
    "    # Charger les données d'entraînement depuis les chemins\n",
    "    X_train = np.load(X_train_path, allow_pickle=True)\n",
    "    y_train = np.load(y_train_path, allow_pickle=True)\n",
    "\n",
    "    # Définir les modèles disponibles\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Support Vector Machine': SVC(),\n",
    "        'k-Nearest Neighbors': KNeighborsClassifier(),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'XGBoost': XGBClassifier()\n",
    "    }\n",
    "\n",
    "    # Définir les distributions de recherche pour chaque modèle\n",
    "    param_distributions = {\n",
    "    'Logistic Regression': {'C': [1.0]},  # Test with C=1.0\n",
    "    'Random Forest': {'n_estimators': [100], 'max_depth': [10]},  # Test with n_estimators=100, max_depth=10\n",
    "    'Support Vector Machine': {'C': [1.0], 'gamma': ['scale']},  # Test with C=1.0 and gamma='scale'\n",
    "    'k-Nearest Neighbors': {'n_neighbors': [5]},  # Test with n_neighbors=5\n",
    "    'Naive Bayes': {},  # No hyperparameters to tune for Naive Bayes\n",
    "    'Decision Tree': {'max_depth': [10]},  # Test with max_depth=10\n",
    "    'XGBoost': {'n_estimators': [100], 'max_depth': [5]}  # Test with n_estimators=100, max_depth=5\n",
    "    }\n",
    "\n",
    "\n",
    "    # Effectuer la recherche des hyperparamètres optimaux pour le modèle choisi\n",
    "    search = RandomizedSearchCV(models[model_choice], param_distributions[model_choice], n_iter=20, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Enregistrer le meilleur modèle trouvé\n",
    "    best_model = search.best_estimator_\n",
    "    with open(f'data/best_{model_choice}.pkl', 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "    # Afficher les hyperparamètres du meilleur modèle\n",
    "    best_params = search.best_params_\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0d4cdda-7046-4d69-894d-f4c07ed6faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_best_model(X_test_path, y_test_path, best_model_path):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, matthews_corrcoef, classification_report\n",
    "    import pickle\n",
    "    \n",
    "    print(\"---- Inside get_metrics_best_model component ----\")\n",
    "    \n",
    "    X_test = np.load(X_test_path, allow_pickle=True)\n",
    "    y_test = np.load(y_test_path, allow_pickle=True)\n",
    "    \n",
    "    # Charger le meilleur modèle\n",
    "    with open(best_model_path, 'rb') as f:\n",
    "        best_model = pickle.load(f)\n",
    "    \n",
    "    # Faire des prédictions avec le meilleur modèle\n",
    "    y_pred_best_model = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculer les probabilités de la classe positive si disponible\n",
    "    if hasattr(best_model, \"predict_proba\"):\n",
    "        y_pred_prob_best_model = best_model.predict_proba(X_test)[:, 1]  # Probabilité de la classe positive\n",
    "        auc = roc_auc_score(y_test, y_pred_prob_best_model)\n",
    "    else:\n",
    "        auc = None\n",
    "\n",
    "    # Calculer les métriques avec les prédictions du meilleur modèle\n",
    "    acc = accuracy_score(y_test, y_pred_best_model)\n",
    "    prec = precision_score(y_test, y_pred_best_model, average='micro')\n",
    "    recall = recall_score(y_test, y_pred_best_model, average='micro')\n",
    "    f1 = f1_score(y_test, y_pred_best_model, average='micro')\n",
    "    cm = confusion_matrix(y_test, y_pred_best_model)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred_best_model)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_best_model))\n",
    "    \n",
    "    print(\"\\nModel Metrics:\")\n",
    "    print(f\"Accuracy: {round(acc, 2)}\")\n",
    "    print(f\"Precision: {round(prec, 2)}\")\n",
    "    print(f\"Recall: {round(recall, 2)}\")\n",
    "    print(f\"F1-Score: {round(f1, 2)}\")\n",
    "    if auc is not None:\n",
    "        print(f\"AUC: {round(auc, 2)}\")\n",
    "    print(f\"MCC: {round(mcc, 2)}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96100616-e817-4424-801d-9a84ca0670a2",
   "metadata": {},
   "source": [
    "### Kubeflow pipeline creation work start from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a4a9257-2348-4db2-84ae-76759dc329d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_step_prepare_data = kfp.components.create_component_from_func(\n",
    "    func=prepare_data,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','seaborn==0.11.2','scikit-learn==0.24.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0b6aea2-18eb-45f1-b51e-85514e5b6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_step_train_test_split = kfp.components.create_component_from_func(\n",
    "    func=train_test_split,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77cb9bca-c243-4a30-b8e2-8da740e4a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_step_training_basic_classifier_with_early_stopping = kfp.components.create_component_from_func(\n",
    "    func=training_basic_classifier_with_early_stopping,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2', 'xgboost==1.4.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daff76d7-b324-4cb8-9fbb-0c5c5058c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_step_predict_on_test_data = kfp.components.create_component_from_func(\n",
    "    func=predict_on_test_data,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2', 'xgboost==1.4.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28cf6745-02f6-4899-b47b-9feb8a6444cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_step_get_metrics = kfp.components.create_component_from_func(\n",
    "    func=get_metrics,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2', 'xgboost==1.4.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcf523bc-1365-484e-8df2-cae40febaebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_step_hyperparameter_tuning = kfp.components.create_component_from_func(\n",
    "    func=hyperparameter_tuning,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2', 'xgboost==1.4.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f304782-61c2-4dd5-b06e-cb06919d5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_step_get_metrics_best_model = kfp.components.create_component_from_func(\n",
    "    func=get_metrics_best_model,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==0.24.2', 'xgboost==1.4.2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6da40632-cc78-4ad8-a524-268d1aaad50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Local clients churn classification pipeline',\n",
    "   description='A pipeline that performs classification to predict the churn of customers'\n",
    ")\n",
    "# Define parameters to be fed into pipeline\n",
    "def churn_classifier_pipeline(data_path: str):\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"t-vol\",\n",
    "    resource_name=\"t-vol\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "\n",
    "    model_choice = 'k-Nearest Neighbors'\n",
    "    model_path = f'/data/{model_choice}.pkl'\n",
    "    best_model_path = f'data/best_{model_choice}.pkl'\n",
    "    X_train_path = f'data/X_train.npy'\n",
    "    X_test_path = f'data/X_test.npy'\n",
    "    y_train_path = f'data/y_train.npy'\n",
    "    y_test_path = f'data/y_test.npy'\n",
    "    y_pred_path = f'data/y_pred.npy'\n",
    "    \n",
    "    prepare_data_task = create_step_prepare_data().add_pvolumes({data_path: vop.volume})\n",
    "    train_test_split = create_step_train_test_split().add_pvolumes({data_path: vop.volume}).after(prepare_data_task)\n",
    "    training_basic_classifier_with_early_stopping = create_step_training_basic_classifier_with_early_stopping(model_choice).add_pvolumes({data_path: vop.volume}).after(train_test_split)\n",
    "    log_predicted_class = create_step_predict_on_test_data(model_path, X_test_path,y_pred_path).add_pvolumes({data_path: vop.volume}).after(training_basic_classifier_with_early_stopping)\n",
    "    log_metrics_task = create_step_get_metrics(y_test_path, y_pred_path).add_pvolumes({data_path: vop.volume}).after(log_predicted_class)\n",
    "    log_hyperparameter_tuning = create_step_hyperparameter_tuning(X_train_path, y_train_path,model_choice).add_pvolumes({data_path: vop.volume}).after(log_metrics_task)\n",
    "    log_metrics_best_model_task = create_step_get_metrics_best_model(X_test_path, y_test_path, best_model_path).add_pvolumes({data_path: vop.volume}).after(log_hyperparameter_tuning)\n",
    "    \n",
    "    prepare_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    train_test_split.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    training_basic_classifier_with_early_stopping.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    log_predicted_class.execution_options.caching_strategy.max_cache_staleness = \"P0D\" \n",
    "    log_metrics_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    log_hyperparameter_tuning.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    log_metrics_best_model_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85840cae-3ca7-4c95-85fd-048de2aa0216",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=churn_classifier_pipeline,\n",
    "    package_path='Churn_Classifier_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f3ef599-74e7-4e86-b659-f8b566cb8c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "#session_cookie = \"MTY2MDY0Mjg0OXxOd3dBTkRSVE5FeElTMEZDVDFVeU5EZE1SMHhUVHpRMU5FcFpNMWRNVWpaTFVrOHlXRFJOVlRReVVFNUxOazFZVEVWQ05FUkZTRUU9fM7IcyOyK49OM0dMDjRJR85gqDksj-YOOLsagNs-_-KR\"\n",
    "# HOST = \"http://localhost:8080/\"\n",
    "# namespace = \"kubeflow\"\n",
    "# client = kfp.Client(\n",
    "#     host=f\"{HOST}/pipeline\",\n",
    "#     #cookies=f\"authservice_session={session_cookie}\",\n",
    "#     namespace=namespace,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6222ec26-e32f-41ae-b9f8-02c9241edd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/5feea4a5-af0d-4f8c-b682-254e8f985090\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/0ad824ca-9da5-438f-bc20-d8b693019809\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_PATH = '/data'\n",
    "\n",
    "import datetime\n",
    "print(datetime.datetime.now().date())\n",
    "\n",
    "\n",
    "pipeline_func = churn_classifier_pipeline\n",
    "experiment_name = 'churn_classifier_exp' +\"_\"+ str(datetime.datetime.now().date())\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "namespace = \"kubeflow\"\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)\n",
    "\n",
    "# from kubernetes import client as k8s_client\n",
    "# pipeline_conf = kfp.dsl.PipelineConf()\n",
    "# pipeline_conf.set_image_pull_secrets([k8s_client.V1ObjectReference(namespace='kubeflow', \n",
    "#                                                                                  name=\"secret\")])\n",
    "# pipeline_conf.set_image_pull_policy(\"IfNotPresent\")\n",
    "    \n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "# kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "#   '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "# run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "#                                                   experiment_name=experiment_name, \n",
    "#                                                   run_name=run_name, \n",
    "#                                                   arguments=arguments,\n",
    "#                                                   namespace = namespace,\n",
    "#                                                   pipeline_conf=pipeline_conf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
